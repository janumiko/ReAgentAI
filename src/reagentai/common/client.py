from collections.abc import Sequence
import logging
from typing import Any

from pydantic_ai import Agent, Tool
from pydantic_ai.tools import AgentDepsT

from src.reagentai.common.mlflow_tracking import MLflowTracker
from src.reagentai.models.llm_output import MultipleOutputs

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class LLMClient:
    def __init__(
        self,
        model_name: str = "google-gla:gemini-2.0-flash",
        tools: Sequence[Tool] = (),
        instructions: str | None = None,
        dependency_types: AgentDepsT | None = None,
        dependencies: Any | None = None,
        mlflow_tracker: MLflowTracker | None = None,
    ):
        """
        Initializes the LLMClient with the specified model, tools, instructions, and dependencies.

        Args:
            model_name (str): The name of the language model to use.
            tools (Sequence[Tool]): A sequence of tools that the agent can use.
            instructions (str | None): Instructions for the agent, if any.
            dependency_types (AgentDepsT | None): Types of dependencies required by the agent.
            dependencies (Any | None): Actual dependencies to be used by the agent.
            mlflow_tracker (MLflowTracker | None): MLflow tracker for experiment tracking.
        """

        self.model_name = model_name
        self.instructions = instructions
        self.tools = tools
        self.dependencies = dependencies
        self.mlflow_tracker = mlflow_tracker or MLflowTracker()

        self.agent = Agent(
            model_name,
            tools=tools,
            instructions=instructions,
            deps_type=dependency_types,
            output_type=MultipleOutputs,
        )

        self.result_history = None
        logger.info(f"LLMClient initialized with model: {model_name}")

        # Log agent initialization
        self.mlflow_tracker.start_run("agent_initialization")
        self.mlflow_tracker.log_params(
            {
                "model_name": model_name,
                "num_tools": len(tools),
                "instructions_length": len(instructions) if instructions else 0,
            }
        )
        self.mlflow_tracker.end_run()

    def set_model(self, model_name: str):
        """
        Sets the model for the LLMClient.
        Args:
            model_name (str): The name of the new language model to use.
        """
        self.model_name = model_name
        self.agent = Agent(
            model_name,
            tools=self.tools,
            instructions=self.instructions,
            deps_type=self.dependencies,
        )
        logger.info(f"LLMClient model set to: {model_name}")

        # Log model change
        self.mlflow_tracker.start_run("model_change")
        self.mlflow_tracker.log_params({"new_model": model_name})
        self.mlflow_tracker.end_run()

    def get_token_usage(self) -> int:
        """
        Returns the token usage of the current agent.

        Returns:
            int: The number of tokens used.
        """
        if self.result_history is not None:
            return self.result_history.usage().total_tokens
        else:
            return 0

    def clear_history(self):
        """
        Clears the chat history of the agent.
        """
        # self.agent.clear_history()
        logger.info("LLMClient chat history cleared.")
        self.result_history = None

    def respond(self, user_query: str, **kwargs) -> list[dict]:
        """
        Responds to a user query and updates the chat history asynchronously.

        Args:
            user_query (str): The user's query.
            chat_history (List[tuple]): The current chat history.
            **kwargs: Additional keyword arguments to pass to the agent's run method.

        Returns:
            list[dict]: A list of messages generated by the agent in response to the user query.
        """

        # Start an MLflow run for this interaction
        # self.mlflow_tracker.start_run(f"interaction_{hash(user_query)[:8] if hash(user_query) else 'new'}")
        # Convert the hash to a string before slicing
        self.mlflow_tracker.start_run(
            f"interaction_{str(hash(user_query))[:8] if user_query else 'new'}"
        )
        self.mlflow_tracker.log_params(
            {
                "query_length": len(user_query),
            }
        )

        if self.result_history is not None:
            message_history = self.result_history.all_messages()
        else:
            message_history = None

        try:
            # Track token usage before the response
            tokens_before = self.get_token_usage()

            import time

            start_time = time.time()

            result = self.agent.run_sync(
                user_query,
                message_history=message_history,
                deps=self.dependencies,
                **kwargs,
            )

            response_time = time.time() - start_time

            self.result_history = result
            logger.info(f"LLMClient response: {result.output}")
            bot_message = result.output.to_message()

            # Calculate tokens used in this interaction
            tokens_after = self.get_token_usage()
            tokens_used = tokens_after - tokens_before

            # Log metrics to MLflow
            self.mlflow_tracker.log_metrics(
                {
                    "response_time_seconds": response_time,
                    "tokens_used": tokens_used,
                    "total_tokens": tokens_after,
                }
            )

            # Log which tools were used, if any
            tool_calls = []
            if hasattr(result, "tool_calls") and result.tool_calls:
                for tool_call in result.tool_calls:
                    tool_calls.append(tool_call.name)

                self.mlflow_tracker.log_params(
                    {
                        "tools_used": ", ".join(tool_calls),
                        "num_tool_calls": len(tool_calls),
                    }
                )

            self.mlflow_tracker.end_run()
            return bot_message

        except Exception as e:
            self.mlflow_tracker.log_params({"error": str(e), "error_type": type(e).__name__})
            self.mlflow_tracker.end_run()
            raise
